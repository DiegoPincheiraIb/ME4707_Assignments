{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T3_ME4707.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFgfJYMMg53o"
      },
      "source": [
        "# Tarea 3: Redes Neuronales\n",
        "\n",
        "ME4707 - Robótica - Semestre 2021-2\n",
        "\n",
        "Profesor: Juan C. Zagal - Auxiliar: Raimundo Lorca - Laboratorios: Gaspar Fábrega"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Erx-dZsh-Gb"
      },
      "source": [
        "# 1. Introducción\n",
        "\n",
        "Entre los algoritmos más comunes dentro del Machine Learning (ML) se encuentran las Redes Neuronales o Neural Networks (NN) que son algoritmos bioinspirados en la forma en que nuestro cerebro procesa información mediante transmisión de señales eléctricas entre las neuronas. Al igual que la mente de un bebé va aprendiendo al ir experimentando y creciendo, las redes neuronales aprenden del error. En el caso del Aprendizaje Supervisado es posible utilizar conjuntos de entrenamiento donde existen pares de datos de entrada y salida `(X, Y)`, ya conocidos, para que la red aprenda de estos a extraer las características y patrones necesarios para llevar a cabo la tarea deseada, ya sea de clasificación o regresión. \n",
        "\n",
        "De este modo, inicialmente las redes comienzan por predecir resultados totalmente erróneos a partir de los datos entregados. No obstante, a medida que esta va siendo entrenada, va ajustando sus parámetros internos para corregir sus respuestas y acercarse los más posible a los resultados esperados.\n",
        "\n",
        "Las Neural Networks se estructuran en una red de nodos generalmente ordenados por capas, que mediante una serie de ponderaciones y conexiones convierten una serie de inputs X en outputs Y. Dependiendo de como se estructuren las redes neuronales, estas pueden cumplir diferentes funciones, permitiéndoles ser aplicadas en una gran variedad de problemas, desde prevensión de fraudes bancarios hasta detección de tumores mediante radiografías.\n",
        "\n",
        "En esta tarea los problemas se abordarán desde un punto de vista más analítico, acercando a las y los estudiantes al funcionamiento y programación de redes neuronales al observar el desempeño de estas en problemas de distinta índole."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSSicothhB2F"
      },
      "source": [
        "# 2. Formato de entrega\n",
        "La tarea se desarrollará en [Google Colab](https://colab.research.google.com/) en un entorno de Python 3. Debe trabajar sobre este mismo archivo .ipynb completando lo que se requiere en cada problema. Se debe entregar:\n",
        "- Reporte de resultados (en un archivo .pdf) y análisis de estos de acuerdo con las distintas partes de la tarea.\n",
        "- En el archivo .ipynb deben dejar la mejor red que hayan logrado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka_Y-k5pmXFu"
      },
      "source": [
        "# 3. Problema Clasificación ANN\n",
        "\n",
        "Se comenzará por analizar un problema de clasificación bastante simple inspirado en el Neural Network Playground de TensorFlow (https://playground.tensorflow.org). En este caso, los puntos son segmentados o bien, etiquetados, en dos regiones dentro de los cuadrantes que se muestran en el diagrama a continuación. Así, el objetivo de nuestro modelo será clasificar la etiqueta de los puntos dentro de este espacio apartir del par de puntos $(x1, x2)$ como valores de entrada.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/RaimundoLorca/Practico_taller/main/problema_01_ref.png\" height=\"280\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uBnDJKcJZ_w"
      },
      "source": [
        "## 3.1 Dataset\n",
        "En este problema se dispone de una serie de puntos (dataset) en el espacio cartesiano $(x_1, x_2)$, los cuales son clasificados en base a su posición en el espacio. Lea con atención el código a continuación que permite la generación de este dataset `(X, Y)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTKbZ-EQotNa"
      },
      "source": [
        "# generación dataset------\n",
        "\n",
        "# importar librerías\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import cos, sin, pi\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "# cantidad de datos en el dataset\n",
        "data_size = 5000\n",
        "\n",
        "# proporción de ruido en el dataset\n",
        "data_noise = 0.0\n",
        "\n",
        "# inicializar sets de datos\n",
        "# X contiene los puntos cartesianos (x, y)\n",
        "# Y contiene las etiquetas de clasificación de cada punto\n",
        "# donde 1 significa que se encuentra dentro del radio y 0 que no.\n",
        "X = np.zeros( (data_size, 2) )\n",
        "Y = np.zeros( (data_size, 2) )\n",
        "\n",
        "# generar datos al azar\n",
        "for i in np.arange(data_size):\n",
        "  x1 = 2*random.random() - 1\n",
        "  x2 = 2*random.random() - 1\n",
        "\n",
        "  # agregar al set X de puntos\n",
        "  X[i, 0] = x1\n",
        "  X[i, 1] = x2\n",
        "\n",
        "  # agregar etiqueta al set Y\n",
        "  if cos(x1*3*pi)*cos(x2*3*pi) > 0.0:\n",
        "    Y[i, 0] = 0\n",
        "    Y[i, 1] = 1\n",
        "  else:\n",
        "    Y[i, 0] = 1\n",
        "    Y[i, 1] = 0\n",
        "\n",
        "  # agregar ruido al dato (utilizado más adelante)\n",
        "  if random.random() < data_noise:\n",
        "    Y[i, :] = 1 - Y[i, :]\n",
        "\n",
        "# plotear distribución en el dataset\n",
        "plt.figure( figsize=(7,7) )\n",
        "plt.scatter(X[:,0], X[:,1], c=Y[:,0], cmap='plasma', alpha=0.5, s=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jI1-3xiwCxK"
      },
      "source": [
        "## 3.2 Data Splitting\n",
        "\n",
        "Teniendo ya el dataset para el entrenamiento de la NN, se debe dividir este en dos sets: uno de entrenamiento (`training set`) y otro de testing (`testing set`). El primero es utilizado, como su nombre lo indica, en el entrenamiento de la red neuronal; mientras que el segundo es utilizado para evaluar el _accuracy_ de la red ya entrenada.\n",
        "\n",
        "Como la red es evaluada con datos que \"nunca ha visto\" durante el entrenamiento, este resultado permite observar si la red ha logrado generalizar el problema o si se ha _overfitteado_ (aprenderse los datos de entrenamiento de \"memoria\").\n",
        "\n",
        "El _data splitting_ se puede lograr con el siguiente código:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avkm6Zb-xTLA"
      },
      "source": [
        "# importar librerías\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# generar sets de datos de training y testing\n",
        "# la varibale test_size permite controlar la proporción entre los datos de testing y training.\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moJfwvqayZZ3"
      },
      "source": [
        "## 3.3 Model Setup\n",
        "\n",
        "Con los sets de entrenamiento y testing listos se puede dar paso a la configuración y entrenamiento de la red neuronal. Para esto se utilizará la librería `keras` o `tf.keras` de `TensorFlow`. Keras es una API de alto nivel para la creación y el entrenamiento de modelos de deep learning. Está orientada y diseñada para la construcción de modelos de forma modular o en bloques. De este modo, ofrece un framework mucho más amigable e intuitivo para principiantes, a la vez que mantiene un estructura personalizable y versátil que permite a usuarios más avanzados incorporar nuevas ideas.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/cherrerab/deeplearningfallas/master/workshop_02/bin/keras_logo.png\" width=\"400\">\n",
        "\n",
        "Los elemenos básicos para la construcción de un modelo o `keras.Model` consisten en las capas o `layers` del modelo. En este sentido, configurar un modelo en Keras resulta en ir uniendo o conectando capas `keras.layers` de manera secuencial.\n",
        "\n",
        "Para comenzar e introducir el framework de esta librería, construiremos un modelo o red neuronal `Sequential` a partir de únicamente capas `keras.layers.Dense` y otras capas elementales.\n",
        "- https://keras.io/api/layers/\n",
        "- https://keras.io/api/layers/activations/\n",
        "\n",
        "En términos generales, compondremos nuestro modelo de una serie de capas `Dense`, que se encargarán de procesar la información y los patrones de los datos de entrada hasta una última capa `Dense` con únicamente dos nodos que determinarán las etiquetas de los puntos $(x_1, x_2)$ que se ingresen al modelo.\n",
        "A continuación se presenta el código necesario para configurar una red de este tipo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3d8gHTl3HRZ"
      },
      "source": [
        "# importar librerías\n",
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# inicializar modelo keras.Sequential\n",
        "model = Sequential()\n",
        "\n",
        "# ---\n",
        "# primero debemos agregar nuestra capa Input donde debemos especificar\n",
        "# las dimensiones de los datos que se ingresarán al modelo\n",
        "input_dim = ( 2, )\n",
        "model.add( Input( shape=input_dim ) )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Dense.\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "# las keras.layers.Dense reciben la cantidad de nodos o units dentro\n",
        "# de la capa y la función de activación que operarán.\n",
        "# https://keras.io/api/layers/activations/\n",
        "model.add(Dense(units = 12, activation = 'relu'))\n",
        "model.add(Dense(units = 12, activation = 'relu'))\n",
        "\n",
        "\n",
        "# ---\n",
        "# por último debemos configurar nuestra capa de salida\n",
        "# dado que el modelo consiste en uno de clasificación emplearemos\n",
        "# la función softmax, donde cada nodo indicará la probabilidad de que\n",
        "# los datos correspondan a una de las etiquetas o estados de salud.\n",
        "labels_num = 2\n",
        "model.add(Dense(units = labels_num, activation = 'softmax'))\n",
        "\n",
        "# imprimir resumen del modelo\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_68VoSFy3nDn"
      },
      "source": [
        "Parte de este analisis consiste en experimentar con distintas cantidades de capas, nodos e incluso funciones de activación (https://keras.io/activations/). Sientase en libertad de modificar la arquitectura de esta red según le parezca.\n",
        "\n",
        "Para entrenar esta red o bien llamado, modelo, basta compilarlo y entrenarlo con los sets de datos generados anteriormente. En este caso utilizaremos los datos de `testing` como datos de validación para monitorear directamente el desempeño del modelo sobre el conjunto de evaluación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7qcROlC4jJf"
      },
      "source": [
        "# ---\n",
        "# compilar modelo siguiendo como función de pérdida\n",
        "# la categorical crossentropy, 'categorical_crossentropy'\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', patience=20)\n",
        "# ---\n",
        "# realizar rutina de entrenamiento\n",
        "train_history = model.fit(X_train, Y_train,\n",
        "                          batch_size=128, epochs=150,\n",
        "                          validation_data=(X_test, Y_test), callbacks = [es] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjQOjt5Q6xW8"
      },
      "source": [
        "## 3.3 Model Evaluation\n",
        "\n",
        "Para evaluar el desempeño de este modelo de clasificación existen múltiples herramientas:\n",
        "-  Gráfico de función de pérdida: La función de pérdida o _loss function_ es el paramétro que se va optimizando a medida que la red se entrena. La red neuronal va ajustando los pesos de ponderación entre los nodos tal de minimizar este paramétro. En el gráfico de función de perdida se puede visualizar el desempeño del entrenamiento del modelo y además la convergencia entre la curva de `training` y `validation`. Si la curva de `validation` se escapa considerablemente de la curva de `training` esto es un indicador de que el modelo ha sufrido _overfitting_ y por tanto ha perdido generalidad (se aprende los datos de `training` de memoria).\n",
        "\n",
        "- Matriz de confusión: Esta tabla permite comparar las predicciones del modelo versus las etiquetas reales de los datos.\n",
        "\n",
        "- Heat Map o Mapa de Clasificación: En el caso de modelos que operan sobre datos 2D, o en el espacio cartesiano, es posible visualizar el resultado de la red neuronal dentro de todo el dominio de los datos (espacio cartesiano). Así es posible visualizar de forma más directa el criterio de clasificación del modelo.\n",
        "\n",
        "Dentro del github del curso `roboticafcfm` se encuentran implementadas las funciones `plot_loss_function`, `plot_confusion_matrix` y `plot_classification_map` para facilitar el uso de estas herramientas. Para cargarlas dentro del entorno de Colab debe ejecutar el siguiente bloque de código."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-KOch-unAqr"
      },
      "source": [
        "!git clone https://github.com/RaimundoLorca/roboticafcfm2021-2.git\n",
        "%cd /content/roboticafcfm2021-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIi1m3g4HlOE"
      },
      "source": [
        "Una vez importado el github del curso, ejecute el siguiente bloque de código y observe los resultados del modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDQW43jr6jtl"
      },
      "source": [
        "from utils import plot_loss_function\n",
        "from utils import plot_confusion_matrix\n",
        "from utils import plot_classification_map\n",
        "\n",
        "# obtener predicciones de X_test con model.predict\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# para que el resultado nos sea más intuitivo transformaremos\n",
        "# las etiquetas nuevamente a non one-hot-encoding\n",
        "# utilizando np.argmax\n",
        "Y_pred = np.argmax(Y_pred, axis = 1)\n",
        "Y_true = np.argmax(Y_test, axis = 1)\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(train_history)\n",
        "\n",
        "# matriz de confusión\n",
        "plot_confusion_matrix(Y_true, Y_pred,\n",
        "                      target_names=['Azul', 'Amarillo'], figsize=(6, 6))\n",
        "\n",
        "# classification map\n",
        "plot_classification_map(model, (0,1), (0,1), 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8nUdA5p74xQ"
      },
      "source": [
        "Podemos además visualizar la red completa y los pesos entre los nodos de las capas, ejecute la siguiente celda de código oculto (se ha colapsado la pestaña para disminuir el espacio utilizado por esta):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLb-SZZRuORj",
        "cellView": "form"
      },
      "source": [
        "#@title Graficar nodos y pesos de la red:\n",
        "#@markdown El siguiente bloque de código genera un grafico de la red neuronal utilizando la libreria NETWORKX. Simplemente haga click en ejecutar celda para ver sus resultados.\n",
        "#@ \n",
        "#@markdown Para leer el código, haga doble click en este bloque de texto o en \"mostrar codigo\" y se expandirá el contenido\n",
        "\n",
        "# Largo de el modelo (cantidad de pesos)\n",
        "length = len(model.trainable_weights)\n",
        "\n",
        "# Se define una constante para escalar el peso de manera logaritmica\n",
        "e = 0.005\n",
        "\n",
        "# importamos la libreria\n",
        "import networkx as nx\n",
        "\n",
        "# iniciamos el grafo\n",
        "G = nx.Graph()\n",
        "\n",
        "# definimos variable utiles para ubicar de manera correcta los nodos en el plano\n",
        "k1 = 0\n",
        "k2 = 0\n",
        "n = 0\n",
        "\n",
        "# iteramos sobre cada arreglo de pesos\n",
        "for w_i in range(0,length,2):\n",
        "  layer = model.trainable_weights[w_i]\n",
        "\n",
        "  if np.min(layer)<0:\n",
        "    layer = np.log(e+layer-np.min(layer))\n",
        "  else:\n",
        "    layer = np.log(e+layer)\n",
        "\n",
        "  x = layer.shape[0]\n",
        "  k1 = k2\n",
        "  k2+=x\n",
        "  y = layer.shape[1]\n",
        "\n",
        "  # para cada arreglo, se inicializan los nodos y se crean las conexiones entre ellos\n",
        "\n",
        "  for i in range(x):\n",
        "    if w_i == 0:\n",
        "      G.add_node(i,pos=(n,-5+10*i))\n",
        "\n",
        "    if y != 2:\n",
        "      for j in range(y):\n",
        "        G.add_node(j+k2,pos=(n+1,j-10))\n",
        "        G.add_edge(i+k1, j+k2, weight = layer[i][j])\n",
        "    else:\n",
        "      for j in range(y):\n",
        "        G.add_node(j+k2,pos=(n+1,-5+10*j))\n",
        "        G.add_edge(i+k1, j+k2, weight = layer[i][j])\n",
        "      \n",
        "  n+=1\n",
        "\n",
        "# se extrae informacion de posicion de nodos y peso de las conexiones\n",
        "pos=nx.get_node_attributes(G,'pos')\n",
        "edges = G.edges()\n",
        "weights = [G[u][v]['weight'] for u,v in edges]\n",
        "\n",
        "# graficamos\n",
        "plt.figure(figsize=(20,12)) \n",
        "nx.draw(G,pos,with_labels=False, width = weights)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gL7fiL_I5dC"
      },
      "source": [
        "## 3.4 Preguntas\n",
        "\n",
        "- Registre los resultados de esta primera red neuronal \"simple\" en su reporte. Luego, agregue más capas a la red y aumente la cantidad de nodos de estas. También pruebe utilizando distintas funciones de activación. Mejore la red lo más posible procurando maximizar el parámetro _val_acc_ mostrado durante el entrenamiento (mientras más cercano a 1.0 mejor). Registre los resultados de esta red mejorada y comente sobre la razón de este mejor desempeño.\n",
        "- En la sección en que se genera el dataset existe el parámetro _data_noise_, este define el porcentaje de ruido a agregar al dataset. Elija tres valores entre [0.05, 0.5] y entrene su red mejorada con estos tres nuevos sets con ruido. Registre los resultados y comente sobre los cambios en el desempeño (utilice los gráficos).\n",
        "- ¿Qué representa la magnitud de los pesos y cuál es su relación con el input de las conexiones? En específico, para pesos positivos, cercanos a cero y negativos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejor Experimento: Experimento 2"
      ],
      "metadata": {
        "id": "7MOsQC3K5HW9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD-aCtc55FEH"
      },
      "source": [
        "# inicializar modelo keras.Sequential\n",
        "model = Sequential()\n",
        "\n",
        "# ---\n",
        "# primero debemos agregar nuestra capa Input donde debemos especificar\n",
        "# las dimensiones de los datos que se ingresarán al modelo\n",
        "input_dim = ( 2, )\n",
        "model.add( Input( shape=input_dim ) )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Dense.\n",
        "# https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "# las keras.layers.Dense reciben la cantidad de nodos o units dentro\n",
        "# de la capa y la función de activación que operarán.\n",
        "# https://keras.io/api/layers/activations/\n",
        "model.add(Dense(units = 300, activation = 'relu'))\n",
        "model.add(Dense(units = 200, activation = 'relu'))\n",
        "model.add(Dense(units = 150, activation = 'relu'))\n",
        "model.add(Dense(units = 100, activation = 'relu'))\n",
        "model.add(Dense(units = 50, activation = 'relu'))\n",
        "model.add(Dense(units = 25, activation = 'relu'))\n",
        "model.add(Dense(units = 10, activation = 'relu'))\n",
        "\n",
        "\n",
        "# ---\n",
        "# por último debemos configurar nuestra capa de salida\n",
        "# dado que el modelo consiste en uno de clasificación emplearemos\n",
        "# la función softmax, donde cada nodo indicará la probabilidad de que\n",
        "# los datos correspondan a una de las etiquetas o estados de salud.\n",
        "labels_num = 2\n",
        "model.add(Dense(units = labels_num, activation = 'softmax'))\n",
        "\n",
        "# imprimir resumen del modelo\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGefYkRa5FEH"
      },
      "source": [
        "# ---\n",
        "# compilar modelo siguiendo como función de pérdida\n",
        "# la categorical crossentropy, 'categorical_crossentropy'\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_accuracy', patience=20)\n",
        "# ---\n",
        "# realizar rutina de entrenamiento\n",
        "train_history = model.fit(X_train, Y_train,\n",
        "                          batch_size=128, epochs=150,\n",
        "                          validation_data=(X_test, Y_test), callbacks = [es])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0UFSt7e5FEI"
      },
      "source": [
        "# obtener predicciones de X_test con model.predict\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# para que el resultado nos sea más intuitivo transformaremos\n",
        "# las etiquetas nuevamente a non one-hot-encoding\n",
        "# utilizando np.argmax\n",
        "Y_pred = np.argmax(Y_pred, axis = 1)\n",
        "Y_true = np.argmax(Y_test, axis = 1)\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(train_history)\n",
        "\n",
        "# matriz de confusión\n",
        "plot_confusion_matrix(Y_true, Y_pred,\n",
        "                      target_names=['Azul', 'Amarillo'], figsize=(6, 6))\n",
        "\n",
        "# classification map\n",
        "plot_classification_map(model, (0,1), (0,1), 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vz8zMA2j7WCx"
      },
      "source": [
        "#@title Graficar nodos y pesos de la red:\n",
        "#@markdown El siguiente bloque de código genera un grafico de la red neuronal utilizando la libreria NETWORKX. Simplemente haga click en ejecutar celda para ver sus resultados.\n",
        "#@ \n",
        "#@markdown Para leer el código, haga doble click en este bloque de texto o en \"mostrar codigo\" y se expandirá el contenido\n",
        "\n",
        "# Largo de el modelo (cantidad de pesos)\n",
        "length = len(model.trainable_weights)\n",
        "\n",
        "# Se define una constante para escalar el peso de manera logaritmica\n",
        "e = 0.005\n",
        "\n",
        "# importamos la libreria\n",
        "import networkx as nx\n",
        "\n",
        "# iniciamos el grafo\n",
        "G = nx.Graph()\n",
        "\n",
        "# definimos variable utiles para ubicar de manera correcta los nodos en el plano\n",
        "k1 = 0\n",
        "k2 = 0\n",
        "n = 0\n",
        "\n",
        "# iteramos sobre cada arreglo de pesos\n",
        "for w_i in range(0,length,2):\n",
        "  layer = model.trainable_weights[w_i]\n",
        "\n",
        "  if np.min(layer)<0:\n",
        "    layer = np.log(e+layer-np.min(layer))\n",
        "  else:\n",
        "    layer = np.log(e+layer)\n",
        "\n",
        "  x = layer.shape[0]\n",
        "  k1 = k2\n",
        "  k2+=x\n",
        "  y = layer.shape[1]\n",
        "\n",
        "  # para cada arreglo, se inicializan los nodos y se crean las conexiones entre ellos\n",
        "\n",
        "  for i in range(x):\n",
        "    if w_i == 0:\n",
        "      G.add_node(i,pos=(n,-5+10*i))\n",
        "\n",
        "    if y != 2:\n",
        "      for j in range(y):\n",
        "        G.add_node(j+k2,pos=(n+1,j-10))\n",
        "        G.add_edge(i+k1, j+k2, weight = layer[i][j])\n",
        "    else:\n",
        "      for j in range(y):\n",
        "        G.add_node(j+k2,pos=(n+1,-5+10*j))\n",
        "        G.add_edge(i+k1, j+k2, weight = layer[i][j])\n",
        "      \n",
        "  n+=1\n",
        "\n",
        "# se extrae informacion de posicion de nodos y peso de las conexiones\n",
        "pos=nx.get_node_attributes(G,'pos')\n",
        "edges = G.edges()\n",
        "weights = [G[u][v]['weight'] for u,v in edges]\n",
        "\n",
        "# graficamos\n",
        "plt.figure(figsize=(20,12)) \n",
        "nx.draw(G,pos,with_labels=False, width = weights)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimento 2 + ruido"
      ],
      "metadata": {
        "id": "U7OeX01sSuvJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4gIvzOEXcNx"
      },
      "source": [
        "\n",
        "# cantidad de datos en el dataset\n",
        "data_size = 5000\n",
        "\n",
        "\n",
        "lista = [0.07, 0.14, 0.38]\n",
        "for data_noise in lista:\n",
        "\n",
        "  print('\\n Data noise: ', data_noise, '\\n')\n",
        "\n",
        "  # inicializar sets de datos\n",
        "  # X contiene los puntos cartesianos (x, y)\n",
        "  # Y contiene las etiquetas de clasificación de cada punto\n",
        "  # donde 1 significa que se encuentra dentro del radio y 0 que no.\n",
        "  X = np.zeros( (data_size, 2) )\n",
        "  Y = np.zeros( (data_size, 2) )\n",
        "\n",
        "  # generar datos al azar\n",
        "  for i in np.arange(data_size):\n",
        "    x1 = 2*random.random() - 1\n",
        "    x2 = 2*random.random() - 1\n",
        "\n",
        "    # agregar al set X de puntos\n",
        "    X[i, 0] = x1\n",
        "    X[i, 1] = x2\n",
        "\n",
        "    # agregar etiqueta al set Y\n",
        "    if cos(x1*3*pi)*cos(x2*3*pi) > 0.0:\n",
        "      Y[i, 0] = 0\n",
        "      Y[i, 1] = 1\n",
        "    else:\n",
        "      Y[i, 0] = 1\n",
        "      Y[i, 1] = 0\n",
        "\n",
        "    # agregar ruido al dato (utilizado más adelante)\n",
        "    if random.random() < data_noise:\n",
        "      Y[i, :] = 1 - Y[i, :]\n",
        "\n",
        "  # plotear distribución en el dataset\n",
        "  plt.figure( figsize=(7,7) )\n",
        "  plt.scatter(X[:,0], X[:,1], c=Y[:,0], cmap='plasma', alpha=0.5, s=50)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "  # inicializar modelo keras.Sequential\n",
        "  model = Sequential()\n",
        "\n",
        "  # ---\n",
        "  # primero debemos agregar nuestra capa Input donde debemos especificar\n",
        "  # las dimensiones de los datos que se ingresarán al modelo\n",
        "  input_dim = ( 2, )\n",
        "  model.add( Input( shape=input_dim ) )\n",
        "\n",
        "  # ---\n",
        "  # ahora debemos ir agregando nuestras capas Dense.\n",
        "  # https://keras.io/api/layers/core_layers/dense/\n",
        "\n",
        "  # las keras.layers.Dense reciben la cantidad de nodos o units dentro\n",
        "  # de la capa y la función de activación que operarán.\n",
        "  # https://keras.io/api/layers/activations/\n",
        "  model.add(Dense(units = 300, activation = 'relu'))\n",
        "  model.add(Dense(units = 200, activation = 'relu'))\n",
        "  model.add(Dense(units = 150, activation = 'relu'))\n",
        "  model.add(Dense(units = 100, activation = 'relu'))\n",
        "  model.add(Dense(units = 50, activation = 'relu'))\n",
        "  model.add(Dense(units = 25, activation = 'relu'))\n",
        "  model.add(Dense(units = 10, activation = 'relu'))\n",
        "\n",
        "\n",
        "  # ---\n",
        "  # por último debemos configurar nuestra capa de salida\n",
        "  # dado que el modelo consiste en uno de clasificación emplearemos\n",
        "  # la función softmax, donde cada nodo indicará la probabilidad de que\n",
        "  # los datos correspondan a una de las etiquetas o estados de salud.\n",
        "  labels_num = 2\n",
        "  model.add(Dense(units = labels_num, activation = 'softmax'))\n",
        "\n",
        "  # imprimir resumen del modelo\n",
        "  model.summary()\n",
        "\n",
        "  # ---\n",
        "  # compilar modelo siguiendo como función de pérdida\n",
        "  # la categorical crossentropy, 'categorical_crossentropy'\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  es = EarlyStopping(monitor='val_accuracy', patience=20)\n",
        "  # ---\n",
        "  # realizar rutina de entrenamiento\n",
        "  train_history = model.fit(X_train, Y_train,\n",
        "                            batch_size=128, epochs=150,\n",
        "                            validation_data=(X_test, Y_test), callbacks = [es])\n",
        "\n",
        "  from utils import plot_loss_function\n",
        "  from utils import plot_confusion_matrix\n",
        "  from utils import plot_classification_map\n",
        "\n",
        "  # obtener predicciones de X_test con model.predict\n",
        "  Y_pred = model.predict(X_test)\n",
        "\n",
        "  # para que el resultado nos sea más intuitivo transformaremos\n",
        "  # las etiquetas nuevamente a non one-hot-encoding\n",
        "  # utilizando np.argmax\n",
        "  Y_pred = np.argmax(Y_pred, axis = 1)\n",
        "  Y_true = np.argmax(Y_test, axis = 1)\n",
        "\n",
        "  # plot gráfico de función de pérdida\n",
        "  plot_loss_function(train_history)\n",
        "\n",
        "  # matriz de confusión\n",
        "  plot_confusion_matrix(Y_true, Y_pred,\n",
        "                        target_names=['Azul', 'Amarillo'], figsize=(6, 6))\n",
        "\n",
        "  # classification map\n",
        "  plot_classification_map(model, (0,1), (0,1), 256)\n",
        "\n",
        "\n",
        "  print('\\n')\n",
        "  print('--------------------------------------------\\n')\n",
        "  print('--------------------------------------------\\n')\n",
        "  print('--------------------------------------------\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TsASAJAaWsq"
      },
      "source": [
        "# 4. Problema Clasificación CNN\n",
        " \n",
        "A partir del problema anterior, es claro que las redes neuronales son capaces de resolver problemas de clasificación simples, incluso ante la presencia de algo de ruido. La pregunta entonces radica si son capaces de resolver problemas de clasificación más complejos, como por ejemplo, clasificación de imágenes digitales.\n",
        "\n",
        "En este problema se trabajará con un dataset que contiene cerca de 60.000 imágenes de `32x32px` correspondientes a 10 clases distintas (perros, ranas, barcos, camiones, etc.) llamado CIFAR-10. El objetivo de este problema es desarrollar una Red Neuronal Convolucional o Convolutional Neural Network (CNN) capaz de discernir entre las distintas clases y lograr clasificar correctamente cada imagen de acuerdo a su clase correspondiente.\n",
        "\n",
        "Mas información sobre el dataset: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/944/1*6XQqOifwnmplS22zCRRVaw.png\" height=\"250\"> <img src=\"https://raw.githubusercontent.com/RaimundoLorca/Practico_taller/main/problema_02_ref.png\" height=\"250\">\n",
        "\n",
        "TIP: Para que el modelo no demore tanto en entrenarse, es conveniente cambiar el tipo de entorno de ejecución a uno acelerado de GPU. Esto lo pueden hacer en Entorno de Ejecución --> Cambiar Tipo de Entorno de Ejecución --> Acelerador de hardware --> GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vrPid3sfUXu"
      },
      "source": [
        "## 4.1 Dataset\n",
        "\n",
        "El dataset CIFAR-10 puede ser obtenido corriendo el código a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLn-dQ1D0eRo"
      },
      "source": [
        "# importar librerías\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "# importar sets de training y testing\n",
        "(X_train, Y_train_non_one_hot), (X_test, Y_test_non_one_hot) = cifar10.load_data()\n",
        "\n",
        "# ---\n",
        "# visualizar algunas imágenes dentro del dataset\n",
        "imgs = np.hstack( [X_train[0,:,:], X_train[100,:,:],\n",
        "                   X_train[217,:,:], X_train[500,:,:]] )\n",
        "\n",
        "plt.figure( figsize=(10,10) )\n",
        "plt.imshow(imgs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OavprQGiiEXJ"
      },
      "source": [
        "Como se puede observar, el dataset ya se encuentra separado en sets de training y testing. No obstante, es necesario algo de procesamiento previo al entrenamiento. Corra el bloque de código a continuación para preparar los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_cmSDIKiEGJ"
      },
      "source": [
        "# importar librerías\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# ---\n",
        "# dado que las etiquetas no vienen con one-hot encoding las transformamos a estas.\n",
        "# one-hot-encoding consiste en codificar las etiquetas de la forma [0, 0, 1]\n",
        "# cuando, por ejemplo, se tienen tres clases. Así, esta estructura es compatible\n",
        "# con la salida de una capa softmax.\n",
        "\n",
        "# para esto se puede utilizar la función to_categorical de keras.utils\n",
        "Y_train = to_categorical(Y_train_non_one_hot, 10)\n",
        "Y_test = to_categorical(Y_test_non_one_hot, 10)\n",
        "\n",
        "# ---\n",
        "# para un experimento posterior aislaremos las clases correspondientes\n",
        "# a las clases de perros y gatos, donde la etiqueta 5 corresponde a perros\n",
        "# y la etiqueta 3 a gatos.\n",
        "X_dc_train = np.zeros( (10000, 32, 32, 3) )\n",
        "Y_dc_train = np.zeros( (10000, 1) )\n",
        "\n",
        "X_dc_test = np.zeros( (2000, 32, 32, 3) )\n",
        "Y_dc_test = np.zeros( (2000, 1) )\n",
        "\n",
        "\n",
        "n = 0\n",
        "# recorreremos todas la imágenes en cifar-10\n",
        "for i in range(X_train.shape[0]):\n",
        "  # si tiene la etiqueta correspondiente a perro\n",
        "  if (Y_train_non_one_hot[i][0] == 5):\n",
        "    # agregamos la imágen al dataset\n",
        "    X_dc_train[n,:,:,:] = X_train[i,:,:,:]\n",
        "    Y_dc_train[n] = 0\n",
        "    n = n+1\n",
        "\n",
        "  # si tiene la etiqueta correspondiente a gato\n",
        "  elif (Y_train_non_one_hot[i][0] == 3):\n",
        "    X_dc_train[n,:,:,:] = X_train[i,:,:,:]\n",
        "    Y_dc_train[n] = 1\n",
        "    n = n+1\n",
        "\n",
        "# datos testing\n",
        "n = 0\n",
        "for i in range(X_test.shape[0]):\n",
        "  # si tiene la etiqueta correspondiente a perro\n",
        "  if (Y_test_non_one_hot[i][0] == 5):\n",
        "    X_dc_test[n,:,:,:] = X_test[i,:,:,:]\n",
        "    Y_dc_test[n] = 0\n",
        "    n = n+1\n",
        "  # si tiene la etiqueta correspondiente a gato\n",
        "  elif (Y_test_non_one_hot[i][0] == 3):\n",
        "    X_dc_test[n,:,:,:] = X_test[i,:,:,:]\n",
        "    Y_dc_test[n] = 1\n",
        "    n = n+1\n",
        "\n",
        "# pasamos a one hot encoding\n",
        "Y_dc_train = to_categorical(Y_dc_train, 2)\n",
        "Y_dc_test = to_categorical(Y_dc_test, 2)\n",
        "\n",
        "# ---\n",
        "# mostrar ejemplos\n",
        "imgs = np.hstack( [X_dc_train[0,:,:], X_dc_train[100,:,:],\n",
        "                   X_dc_train[200,:,:], X_dc_train[300,:,:]] )\n",
        "\n",
        "plt.figure( figsize=(10,10) )\n",
        "plt.imshow(np.uint8(imgs))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG321nYIkngF"
      },
      "source": [
        "## 4.2 Model Setup\n",
        "En el desarrollo de una red CNN hay tres nuevos tipos de capas a tomar en consideración.\n",
        "- Conv2D (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D): Esta capa en vez de componerse de nodos se compone de filtros (como los vistos en el capítulo de visión computacional), cada uno de estos filtros es convolucionado con la imagen de entrada produciendo una nueva imagen _filtrada_ que pasa a la siguiente capa.\n",
        "- MaxPooling2D (https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D): A medida que avanza la información a través de la red, conviene ir reduciendo el tamaño de las imágenes tal de ir aislando únicamente la información relevante (_features_). Además, así se reduce significativamente el costo computacional durante el entrenamiento. Este tipo de capas reduce las dimensiones de las imágenes que entran a estas.\n",
        "\n",
        "- Flatten (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten): Dado que después de todo este es un problema de clasificación, la última capa debe consistir en un par de nodos softmax que entreguen las etiquetas. Para conectar las imágenes de las capas anteriores a esta capa (o a capas Dense) es necesario agregar una capa Flatten que transforme la información de las imágenes en vectores de 1D.\n",
        "\n",
        "A continuación, se presenta una arquitectura bastante simple de una CNN que cumpla el propósito de este problema. La idea es que, al igual que en el problema anterior, vaya modificando la arquitectura tal de mejorarla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbWw3_o95HP"
      },
      "source": [
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# inicializar modelo keras.Sequential\n",
        "model = Sequential()\n",
        "\n",
        "# ---\n",
        "# primero debemos agregar nuestra capa Input donde debemos especificar\n",
        "# las dimensiones de los datos que se ingresarán al modelo\n",
        "# las capas Conv2D reciben tensores de la forma (height, width, channels)\n",
        "input_dim = ( 32, 32, 3)\n",
        "model.add( Input( shape=input_dim ) )\n",
        "\n",
        "# ---\n",
        "# ahora debemos ir agregando nuestras capas Conv2D y Pooling.\n",
        "\n",
        "# las keras.layers.Conv2D reciben la cantidad de filtros dentro de la capa,\n",
        "# el tamaño de estos filtros y la función de activación con que operarán.\n",
        "# https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "\n",
        "# las keras.layers.MaxPooling2D reciben el tamaño de la ventana sobre\n",
        "# la cual llevarán a cabo el down-sampling\n",
        "# https://keras.io/api/layers/pooling_layers/max_pooling2d/\n",
        "\n",
        "\n",
        "### CAPAS CONVOLUCIONALES (Extracción de características)\n",
        "\n",
        "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(64, kernel_size = (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(64, kernel_size = (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Conv2D(128, kernel_size = (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(128, kernel_size = (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "### CAPAS DENSAS (clasificación)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "\n",
        "model.add(Dense(units = 10, activation = 'softmax'))\n",
        "\n",
        "# imprimir resumen del modelo\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rQiJLbNpo23"
      },
      "source": [
        "Antes de empezar a entrenar el modelo, se almacenarán los parametros entrenables (weights/pesos), con el fin de compararlos antes y después de entrenar la red. (Simplemente corra el bloque de codigo)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvmkJiZUicS-"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "#Se guardan los valores de los parametros (weights) antes \n",
        "#de entrenar el modelo\n",
        "weights_before_training = deepcopy(model.trainable_weights) #Todas las capas\n",
        "weights_before_training_2 = deepcopy(model.trainable_weights[2]) #Solo la segunda capa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZI-vJIa-aQ"
      },
      "source": [
        "Luego, al igual que en el caso anterior, para entrenar esta red o bien llamado, modelo, basta compilarlo y entrenarlo con los sets de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqxF0TS7pzsW"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "# ---\n",
        "# compilar modelo siguiendo como función de pérdida\n",
        "# la categorical crossentropy, 'categorical_crossentropy'\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
        "# ---\n",
        "# realizar rutina de entrenamiento\n",
        "train_history = model.fit(X_train, Y_train,\n",
        "                          batch_size=128, epochs=60,\n",
        "                          validation_data=(X_test, Y_test), callbacks=[es] )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sN2XxBvgjdjm"
      },
      "source": [
        "#Se guardan los valores de los parametros (weights) después \n",
        "#de entrenar el modelo\n",
        "weights_after_training = model.trainable_weights #Todas las capas\n",
        "weights_after_training_2 = model.trainable_weights[2] #Segunda capa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_73_8jCqlBn"
      },
      "source": [
        "## 4.3 Model Evaluation\n",
        "\n",
        "Para evaluar el desempeño de este modelo de clasificación corra el código a continuación.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "ua5vAI8bxOJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqcXm0uGq1JV"
      },
      "source": [
        "from utils import plot_loss_function\n",
        "from utils import plot_confusion_matrix\n",
        "from utils import plot_classification_map\n",
        "\n",
        "# ---\n",
        "# obtener etiquetas predichas a partir del modelo, mediante model.predict\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# para que el resultado nos sea más intuitivo transformaremos\n",
        "# las etiquetas nuevamente a non one-hot-encoding\n",
        "# utilizando np.argmax\n",
        "Y_pred = np.argmax(Y_pred, axis = 1)\n",
        "Y_true = np.argmax(Y_test, axis = 1)\n",
        "\n",
        "# plot gráfico de función de pérdida\n",
        "plot_loss_function(train_history)\n",
        "\n",
        "# matriz de confusión\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "plot_confusion_matrix(Y_true, Y_pred,\n",
        "                      target_names=class_names, figsize=(6, 6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNkrMIrBdJlX"
      },
      "source": [
        "## 4.4 Comparación parametros entrenables\n",
        "\n",
        "Una vez entrenado y evaluado la red, se puede corrobar si los parámetros entrenables del modelo (weights/pesos) realmente fueron modificados. Para lograr esto, se visualizarán estos parámetros para las primeras capas de la red. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgiF0SP-gu5v"
      },
      "source": [
        "### Antes de entrenar\n",
        "En la imagen siguiente se puede ver una representación de los parametros que componen una capa tipo Conv2D, las cuales en vez de componerse de nodos se componen de filtros/kernels. En este caso, cada cuadrado de colores de dimensiones 3x3 corresponde a uno de los filtros, los cuales contienen la información de los parámetros entrenables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dvLKwC8ea_-"
      },
      "source": [
        "#Visualización parametros antes de entrenar la red\n",
        "fig,axs = plt.subplots(32,32, figsize = (12,12))\n",
        "for i in range(32):\n",
        "  for j in range(32):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(weights_before_training_2[:,:,i,j])\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM9RUS1Ihzbo"
      },
      "source": [
        "### Después de entrenar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWGelZ-ee58a"
      },
      "source": [
        "#Visualización parametros después de entrenar la red\n",
        "fig,axs = plt.subplots(32,32, figsize = (12,12))\n",
        "for i in range(32):\n",
        "  for j in range(32):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(weights_after_training_2[:,:,i,j])\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djkYO-e1h7W3"
      },
      "source": [
        "### Otras capas\n",
        "Este bloque de codigo permite corroborar si los parametros en cada una de las capas de la red fueron modificados luego de entrenar el modelo. Si entrega \"False\", entonces los parametros fueron modificados, mientras que si entrega \"True\", los parametros no cambiaron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b143Fr_7fIXK"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#Chequea capa a capa si los parámetros fueron modificados\n",
        "print('Empezar la comparación de pesos entrenables')\n",
        "for i in range(len(weights_before_training)):\n",
        "    print(f'Los parámetros de la capa {i + 1} son iguales:', \n",
        "      tf.reduce_all(tf.equal(weights_before_training[i], weights_after_training[i])).numpy())\n",
        "print('Fin de la comparación de pesos entrenables')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wanKa_9Q0Pho"
      },
      "source": [
        "## 4.5 Explorando la red\n",
        "\n",
        "Es posible observar y analizar el comportamiento de nuestra red, observando lo que hace cada una de las capas que configuramos, para ello, debemos generar un modelo nuevo que devuelva como output la evaluación de las capas superiores en vez de el resultado final.\n",
        "\n",
        "Nos vamos a enfocar en las capas que estudian las imágenes de manera matricial, es decir, las capas bidimensionales:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVeK4Fk3qfVx"
      },
      "source": [
        "layers_outputs = [layer.output for layer in model.layers[:6]]\n",
        "activation_model = keras.Model(inputs = model.input, outputs = layers_outputs)\n",
        "activations = activation_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k5QgE_S0jeJ"
      },
      "source": [
        "Una vez definido, podemos ver si funciona de manera correcta, mostrando su resumen y asegurandonos de que devuelva la cantidad de capas correctas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLHcu9fer3MQ"
      },
      "source": [
        "print(\"len(activations) = \",len(activations))\n",
        "activation_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkJdqaY105qG"
      },
      "source": [
        "Una vez confirmado que la definimos de manera correcta, comenzamos a graficar las capas para poder observar sus parametros y que operacion realiza cada nodo sobre la imagen de entrada:\n",
        "\n",
        "### CAPA 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifrnu09qtRvp"
      },
      "source": [
        "# Elige una imagen para ver como es interpretada por la red\n",
        "img = 3000\n",
        "print(\"predicción = \", class_names[Y_pred[img]])\n",
        "print(\"Clasificacion real = \", class_names[Y_true[img]])\n",
        "\n",
        "# Seleccionar la capa (indices en python parten del 0)\n",
        "l = 0\n",
        "\n",
        "# Mostrar la imagen original\n",
        "plt.imshow(X_test[img])\n",
        "plt.title(\"IMAGEN ORIGINAL\")\n",
        "plt.show()\n",
        "\n",
        "# Extraer informacion de la capa seleccionada\n",
        "layer_activation = activations[l][img]\n",
        "\n",
        "# Graficar informacion almacenada en los nodos de la capa\n",
        "fig,axs = plt.subplots(4,8, figsize = (15,8))\n",
        "# fig.suptitle(\"LAYER \"+str(l), fontsize = 15)\n",
        "for i in range(4):\n",
        "  for j in range(8):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(layer_activation[:,:,i*4+j], cmap = \"gray\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORuHG19waDaW"
      },
      "source": [
        "### CAPA 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVLD650gySbQ"
      },
      "source": [
        "l = 1\n",
        "\n",
        "layer_activation = activations[l][img]\n",
        "\n",
        "fig,axs = plt.subplots(4,8, figsize = (15,8))\n",
        "# fig.suptitle(\"LAYER \"+str(l), fontsize = 15)\n",
        "for i in range(4):\n",
        "  for j in range(8):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(layer_activation[:,:,i*4+j], cmap = \"gray\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4_BsLXzaG9t"
      },
      "source": [
        "### CAPA 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_9_wHQPyfHP"
      },
      "source": [
        "l = 2\n",
        "\n",
        "layer_activation = activations[l][img]\n",
        "\n",
        "print(layer_activation.shape)\n",
        "\n",
        "fig,axs = plt.subplots(4,8, figsize = (15,8))\n",
        "# fig.suptitle(\"LAYER \"+str(l), fontsize = 15)\n",
        "for i in range(4):\n",
        "  for j in range(8):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(layer_activation[:,:,i*4+j], cmap = \"gray\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdNCKAZKaKk1"
      },
      "source": [
        "### CAPA 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpMCrHSNzNWf"
      },
      "source": [
        "l = 3\n",
        "\n",
        "layer_activation = activations[l][img]\n",
        "\n",
        "print(layer_activation.shape)\n",
        "\n",
        "fig,axs = plt.subplots(6,8, figsize = (15,12))\n",
        "# fig.suptitle(\"LAYER \"+str(l), fontsize = 15)\n",
        "for i in range(6):\n",
        "  for j in range(8):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(layer_activation[:,:,i*4+j], cmap = \"gray\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-ISb_GTaNF9"
      },
      "source": [
        "### CAPA 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2vVNcf6zihg"
      },
      "source": [
        "l = 4\n",
        "\n",
        "layer_activation = activations[l][img]\n",
        "\n",
        "print(layer_activation.shape)\n",
        "\n",
        "fig,axs = plt.subplots(6,8, figsize = (15,12))\n",
        "# fig.suptitle(\"LAYER \"+str(l), fontsize = 15)\n",
        "for i in range(6):\n",
        "  for j in range(8):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(layer_activation[:,:,i*4+j], cmap = \"gray\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ4FHortaOk-"
      },
      "source": [
        "### CAPA 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSwepk9ez2Eh"
      },
      "source": [
        "l = 5\n",
        "\n",
        "layer_activation = activations[l][img]\n",
        "\n",
        "print(layer_activation.shape)\n",
        "\n",
        "fig,axs = plt.subplots(6,8, figsize = (15,12))\n",
        "# fig.suptitle(\"LAYER \"+str(l), fontsize = 15)\n",
        "for i in range(6):\n",
        "  for j in range(8):\n",
        "    axs[i,j].axis(\"off\")\n",
        "    axs[i,j].imshow(layer_activation[:,:,i*4+j], cmap = \"gray\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8-HsAtsrLoV"
      },
      "source": [
        "## 4.6 Preguntas\n",
        "- Modifique la arquitectura de la red hasta mejorarla lo más posible (un _val_acc_ sobre 0.6 o 0.7 es suficiente). Agregue a su reporte un esquema de la arquitectura de su red e incluya los gráficos resultantes. Comente sobre el desempeño en la clasificación. ¿Entre que clases suele confundirse más? ¿Tienen sentido estas confusiones?\n",
        "- Comente sobre la accion que realiza cada capa y cada nodo de esta sobre la imagen, es posible observar alguna operación vista en visión computacional?\n",
        "- Compare el efecto de modificar la red sobre las operaciones que realiza cada capa, se observa algun patron? cambia la función de cada capa?\n",
        "- Ahora, cree otra red que trabaje con los sets (X_dc_train, X_dc_test, Y_dc_train y Y_dc_test) para desarrollar una red que clasifique entre perros y gatos. Trate de mejorarla lo más posible y comente sobre los resultados.\n"
      ]
    }
  ]
}